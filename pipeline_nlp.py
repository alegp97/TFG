import os
import json
import shutil
import traceback
import gc
import time
from datetime import datetime

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col

from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import SQLTransformer

import sparknlp
from sparknlp.pretrained import ResourceDownloader

# Spark NLP - base & preprocesamiento
from sparknlp.base import DocumentAssembler, Finisher, TokenAssembler
from sparknlp.annotator import (
    SentenceDetector,
    SentenceDetectorDLModel,
    WordEmbeddingsModel,
    Tokenizer,
    Normalizer,
    StopWordsCleaner,
    Stemmer,
    LemmatizerModel,
    LanguageDetectorDL,
    ContextSpellCheckerModel,
    DocumentNormalizer,
    MarianTransformer,
    NerDLModel,
    NerConverter
)

# Flask
from flask import current_app
from flask_socketio import SocketIO

# Configuraci√≥n personalizada
from settings import SAVE_ALWAYS_AS_PARQUET


class ModelManager:
    def __init__(self, base_path="models_cache"):
        self.base_path = os.path.abspath(base_path)
        os.makedirs(self.base_path, exist_ok=True)
        self.cache = {}

    def get_model_path(self, model_name: str, lang: str = "en") -> str:
        """Devuelve la ruta local donde se guardar√° el modelo."""
        safe_model_name = model_name.replace("/", "_")  # Evitar errores con nombres con "/"
        return os.path.join(self.base_path, f"{safe_model_name}_{lang}")

    def download_and_cache(self, model_class, model_name: str, lang: str = "en"):
        """Carga desde cach√© en memoria, disco o descarga si es necesario."""
        cache_key = f"{model_class.__name__}:{model_name}:{lang}"

        # ‚úÖ En memoria
        if cache_key in self.cache:
            return self.cache[cache_key]

        model_path = self.get_model_path(model_name, lang)

        # üìÇ Ya existe en disco
        if os.path.exists(model_path):
            model = model_class.load(model_path)
        else:
            # üåê Descargar y guardar en disco
            model = model_class.pretrained(model_name, lang)
            os.makedirs(model_path, exist_ok=True)
            model.write().save(model_path)

        self.cache[cache_key] = model
        return model




class PipelineNLP:
    def __init__(self, source, input_data, spark_config, pipeline_config, debug=False):
        """
        Inicializa la sesi√≥n de Spark y carga los datos seg√∫n la fuente (SQL o local).
        
        :param source: 'sql' para base de datos o 'local' para archivos locales.
        :param input_data: La consulta SQL o la ruta del archivo.
        :param spark_config: Configuraci√≥n de Spark en JSON.
        :param pipeline_config: Configuraci√≥n del modelo en JSON.
        :param debug: Booleano para imprimir logs.
        """
        self.source = source
        self.input_data = input_data
        self.spark_config = spark_config
        self.pipeline_config = pipeline_config
        self.debug = debug

        self.model_manager = ModelManager(base_path="models_nlp_local")

        self.execution_metadata = {
            "start_time": None,
            "end_time": None,
            "duration": None,
            "input_type": source,
            "pipeline_config": pipeline_config,
            "spark_config": spark_config,
            "stages_executed": [],
            "models_loaded": [],
            "input_schema": None,
            "input_rows": None,
            "partitions_before": None,
            "partitions_after": None,
            "executors_memory": {},
            "read_time_sec": None,
            "write_time_sec": None,
            "stage_timings": [],
            "error": None
        }
    
    
    def init_spark_session(self):
        """Inicializa la sesi√≥n de Spark NLP con una configuraci√≥n completamente din√°mica desde un JSON."""
        spark_config = self.spark_config["spark"]
        socketio = current_app.extensions["socketio"]

        # Cerrar sesi√≥n anterior si existe
        try:
            existing_spark = SparkSession.getActiveSession()
            if existing_spark is not None:
                print("üîÑ Cerrando sesi√≥n de Spark existente...")
                existing_spark.stop()
            if SparkContext._active_spark_context is not None:
                print("üö® Forzando eliminaci√≥n de SparkContext...")
                SparkContext._active_spark_context.stop()
                SparkContext._active_spark_context = None

            # Forzar la recolecci√≥n de basura para eliminar restos de sesiones previas
            gc.collect()
        except Exception as e:
            print(f"‚ö†Ô∏è No se pudo detener la sesi√≥n previa: {e}")

        try:

            app_name = spark_config.get("app_name", "SparkNLP_App") or "SparkNLP_App"
            spark_builder = SparkSession.builder.appName(app_name)

            # Aplicar configuraciones generales (excepto configurations)
            if "master" in spark_config:
                spark_builder = spark_builder.master(spark_config["master"])

            for key, value in spark_config.items():
                if key not in ["configurations", "app_name", "master"]:
                    spark_builder = spark_builder.config(f"spark.{key}", value)

            # Extraer configuraciones avanzadas de Spark
            extra_configs = spark_config.get("configurations", {})

            # Aplicar configuraciones al builder
            for key, value in extra_configs.items():
                spark_builder = spark_builder.config(key, value)

            # A√±adir Limpiar spark.local.dir
            spark_tmp_path = "/tmp/spark_temp"
            if os.path.exists(spark_tmp_path):
                shutil.rmtree(spark_tmp_path)
            spark_builder = spark_builder.config("spark.local.dir", spark_tmp_path)


            # Crear la sesi√≥n de Spark con la configuraci√≥n completa
            self.spark = spark_builder.getOrCreate()

            # Iniciar Spark NLP sobre la sesi√≥n de Spark creada
            self.spark = sparknlp.start(self.spark)

            # Depuraci√≥n: Mostrar la configuraci√≥n final de Spark NLP si est√° en modo debug
            if self.debug:
                logs = []
                socketio.emit("pipeline_output", {"message": "‚öôÔ∏è Configuraci√≥n final de Spark NLP:"})
                for key, value in self.spark.sparkContext.getConf().getAll():
                    log_message = f"üîπ {key} = {value}"
                    socketio.emit("pipeline_output", {"message": log_message})
                    logs.append(log_message)
                return "\n".join(logs)
            else:
                return None
            
        except Exception as e:
            # Obtener el traceback completo para m√°s detalles
            error_trace = traceback.format_exc()

            # Enviar detalles al WebSocket para depuraci√≥n en tiempo real
            error_message = f"‚ùå Error durante la inicializaci√≥n de Spark NLP: {str(e)}"
            socketio.emit("pipeline_output", {"message": error_message})
            socketio.emit("pipeline_output", {"message": f"üìú Detalles t√©cnicos:\n{error_trace}"})

            # Imprimir en consola para logs adicionales
            print(error_message)
            print(error_trace)

            # Lanzar la excepci√≥n con m√°s contexto
            raise RuntimeError(f"Spark NLP Initialization Failed:\n{error_trace}")
    


    def get_spark_session(self):
        """Devuelve la sesi√≥n de Spark actual."""
        return self.spark

    def stop_pipeline(self):
        """Detiene la sesi√≥n de Spark."""
        if self.spark:
            self.spark.catalog.clearCache()
            self.spark.stop()
            self.spark = None
            print("üßπ SparkSession limpiada y cerrada.")


    # def apply_nlp_pipeline(self, df):
    #     """Aplica el modelo NLP en Spark"""
    #     if self.debug:
    #         print("‚öôÔ∏è Aplicando modelo de NLP...")

    #     pipeline_model_path = self.pipeline_config["models"].get("pipeline_path")
    #     if not pipeline_model_path:
    #         raise ValueError("No se especific√≥ un modelo en la configuraci√≥n.")

    #     pipeline_model = PipelineModel.load(pipeline_model_path)
    #     transformed_df = pipeline_model.transform(df)

    #     return transformed_df
    
    def get_execution_metadata(self):
        """Devuelve los metadatos de ejecuci√≥n, asegurando que todo sea JSON serializable."""
        from copy import deepcopy

        # Crear una copia para no modificar el original
        metadata = deepcopy(self.execution_metadata)

        # Formatear tiempos a ISO si existen
        for key in ["start_time", "end_time"]:
            if isinstance(metadata.get(key), datetime):
                metadata[key] = metadata[key].isoformat()

        # Formatear todos los floats a 2 decimales (si quieres un output limpio)
        for key in ["duration", "read_time_sec", "write_time_sec"]:
            if isinstance(metadata.get(key), float):
                metadata[key] = round(metadata[key], 2)

        # Formatear los tiempos por etapa
        if "stage_timings" in metadata:
            for stage in metadata["stage_timings"]:
                if isinstance(stage.get("duration_sec"), float):
                    stage["duration_sec"] = round(stage["duration_sec"], 2)

        return metadata

    
    # def calculate_mean_executor_metadata(self):
    #     exec_mem = self.spark.sparkContext._jsc.sc().getExecutorMemoryStatus()
    #     total_mem = 0
    #     free_mem = 0
    #     count = 0

    #     # ‚úÖ Usamos .entrySet() y lo iteramos con .iterator()
    #     iterator = exec_mem.entrySet().iterator()

    #     while iterator.hasNext():
    #         entry = iterator.next()
    #         host = entry.getKey()
    #         value = entry.getValue()
    #         mem_total = value._1()
    #         mem_free = value._2()
    #         total_mem += mem_total
    #         free_mem += mem_free
    #         count += 1

    #     if count > 0:
    #         avg_total = round((total_mem / count) / (1024 ** 2), 2)
    #         avg_free = round((free_mem / count) / (1024 ** 2), 2)
    #         avg_used = round((total_mem - free_mem) / count / (1024 ** 2), 2)

    #         self.execution_metadata["executors_memory"] = {
    #             "avg_total_MB": avg_total,
    #             "avg_free_MB": avg_free,
    #             "avg_used_MB": avg_used,
    #             "num_executors": count
    #         }
    #     else:
    #         self.execution_metadata["executors_memory"] = {
    #             "error": "No executor memory data found"
    #         }


    def load_from_sql(self):
        """Carga los datos desde una base de datos SQL utilizando la consulta proporcionada."""
        if self.debug:
            print("üì° Cargando datos desde SQL...")

        db_config = self.input_data["database"]
        sql_query = self.input_data["query"]["sql"]

        db_type = db_config["type"]
        host = db_config["host"]
        port = db_config["port"]
        dbname = db_config["dbname"]
        user = db_config["user"]
        password = db_config["password"]

        if db_type == "postgresql":
            url = f"jdbc:postgresql://{host}:{port}/{dbname}"
            driver = "org.postgresql.Driver"
        elif db_type == "mysql":
            url = f"jdbc:mysql://{host}:{port}/{dbname}"
            driver = "com.mysql.cj.jdbc.Driver"
        elif db_type == "sqlserver":
            url = f"jdbc:sqlserver://{host}:{port};databaseName={dbname}"
            driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
        else:
            raise ValueError("Base de datos no soportada")

        if self.debug:
            print(f"Consulta SQL: {sql_query}")

        start_time = time.time()

        df = self.spark.read.format("jdbc").options(
            url=url,
            query=sql_query,
            user=user,
            password=password,
            driver=driver
        ).load()

        end_time = time.time()
        self.execution_metadata["read_time_sec"] = round(end_time - start_time, 3)

        if self.debug:
            print("‚úÖ Datos cargados correctamente desde SQL")
            df.show(5)

        return df


    def load_from_local(self, file_path: str, format: str = None, delimiter: str = ",") -> DataFrame:
        """
        üì• Carga un archivo desde el sistema de archivos local en un DataFrame de Spark.

        Par√°metros:
            - file_path (str): Ruta del archivo de entrada.
            - format (str, opcional): Formato del archivo. Si es None, se infiere por la extensi√≥n.
            - delimiter (str, opcional): Separador utilizado en archivos CSV y TXT. Predeterminado: ",".

        Retorna:
            - DataFrame: DataFrame de Spark con los datos cargados.
        """

        try:
            socketio = current_app.extensions["socketio"]
            socketio.emit("pipeline_output", {"message": f"üì• Cargando archivo desde {file_path}..."})

            if format is None:
                format = os.path.splitext(file_path)[-1].lower().replace(".", "")

            start_time = time.time()

            if format in ["csv", "txt"]:
                df = self.spark.read.option("header", "true").option("inferSchema", "true").option("delimiter", delimiter).csv(file_path)
            elif format == "json":
                df = self.spark.read.json(file_path)
            elif format == "parquet":
                df = self.spark.read.parquet(file_path)
            elif format == "avro":
                df = self.spark.read.format("avro").load(file_path)
            elif format == "orc":
                df = self.spark.read.orc(file_path)
            elif format in ["xls", "xlsx"]:
                df = self.spark.read.format("com.crealytics.spark.excel") \
                    .option("header", "true") \
                    .option("inferSchema", "true") \
                    .load(file_path)
            else:
                raise ValueError(f"‚ùå Formato '{format}' no soportado. Usa 'csv', 'txt', 'json', 'parquet', 'avro', 'orc', 'xls', 'xlsx'.")

            end_time = time.time()
            self.execution_metadata["read_time_sec"] = round(end_time - start_time, 3)

            return df

        except Exception as e:
            socketio.emit("pipeline_output", {"message": f"‚ùå Error al cargar el archivo: {str(e)}"})
            raise e

    def save_to_local(self, df: DataFrame, output_path: str, format: str = None, mode: str = "overwrite", delimiter: str = ","):
        """
        Guarda un DataFrame de Spark en un archivo local en distintos formatos.

        Par√°metros:
            - df (DataFrame): DataFrame de Spark a guardar.
            - output_path (str): Ruta donde se guardar√° el archivo.
            - format (str, opcional): Formato del archivo de salida. Si es None, se infiere por la extensi√≥n.
            - mode (str, opcional): Modo de guardado ('overwrite', 'append', 'error', 'ignore'). Predeterminado: 'overwrite'.
            - delimiter (str, opcional): Separador utilizado en archivos CSV y TXT. Predeterminado: ",".

        Retorna:
            - None
        """
        try:
            if SAVE_ALWAYS_AS_PARQUET:
                format = "parquet"

            socketio = current_app.extensions["socketio"]

            if format is None:
                format = os.path.splitext(output_path)[-1].lower().replace(".", "")

            start_time = time.time()

            if format in ["csv", "txt"]:
                df.write.option("header", "true").option("delimiter", delimiter).mode(mode).csv(output_path)
            elif format == "json":
                df.write.mode(mode).json(output_path)
            elif format == "parquet":
                df.write.mode(mode).parquet(output_path)
            elif format == "avro":
                df.write.format("avro").mode(mode).save(output_path)
            elif format == "orc":
                df.write.mode(mode).orc(output_path)
            elif format in ["xls", "xlsx"]:
                df.write.format("com.crealytics.spark.excel") \
                    .option("header", "true") \
                    .mode(mode) \
                    .save(output_path)
            elif format == "delta":
                extensions = self.spark.conf.get("spark.sql.extensions", "")
                catalog = self.spark.conf.get("spark.sql.catalog.spark_catalog", "")
                if "io.delta.sql.DeltaSparkSessionExtension" not in extensions or \
                "org.apache.spark.sql.delta.catalog.DeltaCatalog" not in catalog:
                    raise EnvironmentError("Delta Lake no est√° habilitado en la sesi√≥n de Spark.")

                df.write.format("delta").mode(mode).save(output_path)
            else:
                raise ValueError(f"‚ùå Formato '{format}' no soportado. ")

            end_time = time.time()
            self.execution_metadata["write_time_sec"] = round(end_time - start_time, 3)

        except Exception as e:
            socketio.emit("pipeline_output", {"message": f"‚ùå Error al guardar el archivo: {str(e)}"})
            raise e

    def run(self, df: DataFrame) -> DataFrame:
        """‚öôÔ∏è Ejecuta el pipeline NLP y emite logs en tiempo real al cliente mediante WebSocket.""" 
        try:
            # üß† Acceder a la instancia de Flask-SocketIO para emitir logs en tiempo real
            socketio = current_app.extensions["socketio"]

            # ‚è≥ Iniciar la recolecci√≥n de m√©tricas
            self.execution_metadata["start_time"] = datetime.now()
            self.execution_metadata["input_rows"] = df.count()
            self.execution_metadata["input_schema"] = df.schema.json()
            self.execution_metadata["partitions_before"] = df.rdd.getNumPartitions()

            stages = []
            cached_models = {}  # Modelos cargados en memoria
            document_columns = []  # Columnas temporales generadas por el pipeline
            model_manager = ModelManager(base_path="models_nlp_local")

            socketio.emit("pipeline_output", {"message": "models_nlp_local"})

            # üîÑ Construcci√≥n din√°mica del pipeline con cada etapa
            # Este bucle es el que recorre todas las etapas definidas en el JSON de configuraci√≥n del pipeline (`self.pipeline_config["stages"]`).
            # Cada etapa debe contener al menos una clave "name" (nombre del transformador/annotator) y opcionalmente "params" con los par√°metros requeridos.
            # Seg√∫n el tipo de etapa (por ejemplo, tokenizer, normalizer, ner, etc.), se inicializa el componente correspondiente de Spark NLP,
            # se configuran sus par√°metros, se a√±ade al pipeline, y se notifican los eventos al frontend mediante `socketio.emit`. 
            for stage in self.pipeline_config["stages"]:
                name = stage.get("name")
                params = stage.get("params", {})

                if not name:
                    socketio.emit("pipeline_output", {"message": "‚ùå Error: Falta 'name' en una etapa."})
                    raise ValueError("Falta 'name' en una etapa.")

                socketio.emit("pipeline_output", {"message": f"üì¢ Agregando etapa: {name} con par√°metros: {params}"})
                stage_start = time.time()


                """
                ////////////////////////////////////////////////////////////////
                        ETAPAS DE TRANSFORMACI√ìN Y PREPROCESAMIENTO
                ////////////////////////////////////////////////////////////////
                """

                # üìÑ Document Assembler
                if name.startswith("document_assembler"):
                    assembler = DocumentAssembler() \
                        .setInputCol(params["inputCol"]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(assembler)
                    document_columns.append(params["outputCol"])

                # üìë Sentence detector
                elif name.startswith("sentence_detector"):
                    sentence_detector = SentenceDetectorDLModel.pretrained("sentence_detector_dl", "xx") \
                        .setInputCols(["document"]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(sentence_detector)
                    document_columns.append(params["outputCol"])

                # üéüÔ∏è Tokenizer
                elif name.startswith("tokenizer"):
                    tokenizer = Tokenizer() \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(tokenizer)
                    document_columns.append(params["outputCol"])
                
                # üß† Word Embeddings
                elif name.startswith("word_embeddings"):
                    embedding_model_name = params.get("pretrained_model", "glove_100d")
                    word_embeddings = WordEmbeddingsModel.pretrained(embedding_model_name, "en") \
                        .setInputCols(params["inputCols"]) \
                        .setOutputCol(params["outputCol"]) \
                        .setEnableInMemoryStorage(True)
                    stages.append(word_embeddings)
                    document_columns.append(params["outputCol"])

                # üßπ Normalizer
                elif name.startswith("normalizer"):
                    normalizer = Normalizer() \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"]) \
                        .setLowercase(params.get("lowercase", True)) \
                        .setCleanupPatterns(params.get("cleanupPatterns", []))
                    stages.append(normalizer)

                # üõë StopWordsCleaner
                elif name.startswith("stopwords_cleaner"):
                    stopwords_cleaner = StopWordsCleaner() \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"]) \
                        .setCaseSensitive(params.get("caseSensitive", False))
                    stages.append(stopwords_cleaner)

                # üå± Stemmer
                elif name.startswith("stemmer"):
                    stemmer = Stemmer() \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(stemmer)

                # üçÉ Lemmatizer
                elif name.startswith("lemmatizer"):
                    lemmatizer = LemmatizerModel.pretrained(params.get("model_name", "lemma_antbnc")) \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(lemmatizer)

                # üó£Ô∏è Detecci√≥n de idioma
                elif name.startswith("language_detector"):
                    language_detector = LanguageDetectorDL.pretrained("ld_wiki_229", "xx") \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(language_detector)

                # üî† Spell Checker (solo si tienes instalado modelos preentrenados)
                elif name.startswith("spell_checker"):
                    spell_checker = ContextSpellCheckerModel.pretrained("spellcheck_dl", "en") \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(spell_checker)

                # üßº Document Normalizer (con expresiones regulares)
                elif name.startswith("document_normalizer"):
                    document_normalizer = DocumentNormalizer() \
                        .setInputCols([params["inputCol"]]) \
                        .setOutputCol(params["outputCol"]) \
                        .setAction(params.get("action", "clean")) \
                        .setPatterns(params.get("patterns", []))
                    stages.append(document_normalizer)
                


                    """
                    ////////////////////////////////////////////////////////////////
                            ETAPAS DE ESTIMACI√ìN - MODELOS PREENTRENADOS
                    ////////////////////////////////////////////////////////////////
                    """

                # üåç Traducci√≥n con Marian Transformer
                elif name.startswith("marian_transformer"):
                    model_name = params["model_name"]
                    input_col = params["inputCol"]
                    output_col = params["outputCol"]

                    # Si el modelo ya est√° en memoria, reutilizarlo sin descargar de nuevo
                    if model_name not in cached_models:
                        socketio.emit("pipeline_output", {"message": f"üåç Descargando modelo de traducci√≥n: {model_name}..."})
                        cached_models[model_name] = MarianTransformer.pretrained(model_name)
                        socketio.emit("pipeline_output", {"message": f"üåç Modelo {model_name} descargado correctamente."})
                    else:
                        socketio.emit("pipeline_output", {"message": f"üåç Modelo {model_name} ya cargado en memoria. Usando cach√©..."})

                    # Crear una nueva instancia del modelo sin descargar de nuevo
                    transformer = MarianTransformer.pretrained(model_name) \
                        .setInputCols([input_col]) \
                        .setOutputCol(output_col)

                    # Agregar al pipeline
                    stages.append(transformer)

                # üîé Named Entity Recognition (NER)
                elif name == "ner_dl":
                    model_name = params["model_name"]
                    input_cols = params.get("inputCols") or [params["inputCol"]]
                    output_col = params["outputCol"]

                    # Si el modelo ya est√° en memoria, reutilizarlo
                    if model_name in cached_models:
                        socketio.emit("pipeline_output", {"message": f"üîç Modelo NER {model_name} ya cargado en memoria. Usando cach√©..."})
                        ner_model = cached_models[model_name]
                    else:
                        # üì• Descargar y cargar el modelo si no est√° en cach√©
                        socketio.emit("pipeline_output", {"message": f"üîç Descargando modelo NER: {model_name}..."})
                        ner_model = NerDLModel.pretrained(model_name, "en")
                        cached_models[model_name] = ner_model
                        socketio.emit("pipeline_output", {"message": f"üîç Modelo NER {model_name} descargado correctamente."})

                    ner_model.setInputCols(input_cols).setOutputCol(output_col)
                    stages.append(ner_model)

                # üîé NER Converter
                elif name == "ner_converter":
                    converter = NerConverter() \
                        .setInputCols(params["inputCols"]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(converter)

                # üèÅ Finisher
                elif name.startswith("finisher"):
                    input_cols = params.get("inputCols", [])
                    output_cols = params.get("outputCols", input_cols)
                    include_metadata = params.get("includeMetadata", False)
                    output_as_array = params.get("outputAsArray", False)

                    finisher = Finisher() \
                        .setInputCols(input_cols) \
                        .setOutputCols(output_cols) \
                        .setIncludeMetadata(include_metadata) \
                        .setOutputAsArray(output_as_array)
                    stages.append(finisher)

                # ‚ö†Ô∏è Etapa no reconocida
                else:
                    socketio.emit("pipeline_output", {"message": f"‚ùå Error: Tipo de etapa '{name}' no soportado."})
                    raise ValueError(f"Tipo de etapa '{name}' no soportado.")

                # ‚è±Ô∏è Guardar tiempo de configuraci√≥n de la etapa (NO ejecuci√≥n real)
                stage_end = time.time()
                self.execution_metadata["stage_timings"].append({
                    "stage": name,
                    "duration_sec": round(stage_end - stage_start, 3)
                })
                self.execution_metadata["stages_executed"].append(name)

            # ‚ùå Verificar si hay etapas v√°lidas
            if not stages:
                socketio.emit("pipeline_output", {"message": "‚ùå Error: No hay etapas v√°lidas en el pipeline."})
                raise ValueError("No hay etapas v√°lidas en el pipeline.")

            # üöÄ Ejecuci√≥n del pipeline
            socketio.emit("pipeline_output", {"message": "üöÄ Ejecutando pipeline en Spark..."})
            nlp_pipeline = Pipeline(stages=stages)

            # üöÄ Ajuste (fit)
            socketio.emit("pipeline_output", {"message": "üöÄ Ajustando (fit)..."} )
            fit_start = time.time()
            model = nlp_pipeline.fit(df)
            df.take(1)  # ‚ö†Ô∏è Forzar ejecuci√≥n del fit
            fit_end = time.time()

            # üöÄ Transformaci√≥n
            socketio.emit("pipeline_output", {"message": "üöÄ Transformando (transform)..."} )
            transform_start = time.time()
            transformed_df = model.transform(df)
            transformed_df.take(1)  # ‚ö†Ô∏è Forzar ejecuci√≥n del transform
            transform_end = time.time()

            # ‚è±Ô∏è Guardar tiempos reales
            self.execution_metadata["stage_timings"].append({
                "stage": "fit", "duration_sec": round(fit_end - fit_start, 3)
            })
            self.execution_metadata["stage_timings"].append({
                "stage": "transform", "duration_sec": round(transform_end - transform_start, 3)
            })

            # üî¢ Particiones finales
            self.execution_metadata["partitions_after"] = transformed_df.rdd.getNumPartitions()

            # # üìä Memoria por ejecutor (media calculada desde el backend Java)
            # self.calculate_mean_executor_metadata()

            # üßπ Eliminar columnas temporales
            for col in document_columns:
                if col in transformed_df.columns:
                    transformed_df = transformed_df.drop(col)

            # üïí Finalizaci√≥n y duraci√≥n total
            self.execution_metadata["end_time"] = datetime.now()
            self.execution_metadata["duration"] = (
                self.execution_metadata["end_time"] - self.execution_metadata["start_time"]
            ).total_seconds()

        except Exception as e:
            # ‚ùå En caso de error, emitir mensaje al cliente y detener la ejecuci√≥n del pipeline
            socketio.emit("pipeline_output", {"message": f"üò± ‚ùå Error durante la ejecuci√≥n del pipeline: {str(e)} ‚ùåüòû"})
            # Obtener el traceback completo para m√°s detalles
            error_trace = traceback.format_exc()

            # Enviar detalles al WebSocket para depuraci√≥n en tiempo real
            error_message = f"‚ùå{str(e)}"
            socketio.emit("pipeline_output", {"message": error_message})
            socketio.emit("pipeline_output", {"message": f"üìú Detalles de traza:\n{error_trace}"})

            # Imprimir en consola para logs adicionales
            print(error_message)
            print(error_trace)

            # Lanzar la excepci√≥n con m√°s contexto
            raise RuntimeError(f"Spark NLP Initialization Failed:\n{error_trace}")

        return transformed_df
