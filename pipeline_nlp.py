import os
import json

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col

from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import SQLTransformer

import sparknlp
from sparknlp.base import DocumentAssembler, Finisher
from sparknlp.annotator import MarianTransformer, NerDLModel

from flask import current_app
from flask_socketio import SocketIO




class PipelineNLP:
    def __init__(self, source, input_data, spark_config, pipeline_config, debug=False):
        """
        Inicializa la sesi√≥n de Spark y carga los datos seg√∫n la fuente (SQL o local).
        
        :param source: 'sql' para base de datos o 'local' para archivos locales.
        :param input_data: La consulta SQL o la ruta del archivo.
        :param spark_config: Configuraci√≥n de Spark en JSON.
        :param pipeline_config: Configuraci√≥n del modelo en JSON.
        :param debug: Booleano para imprimir logs.
        """
        self.source = source
        self.input_data = input_data
        self.spark_config = spark_config
        self.pipeline_config = pipeline_config
        self.debug = debug

    
    
    def init_spark_session(self):
        """Crea la sesi√≥n de Spark con la configuraci√≥n proporcionada."""
        spark_builder = SparkSession.builder.appName(self.spark_config["spark"]["app_name"])

        for key, value in self.spark_config["spark"]["configurations"].items():
            spark_builder = spark_builder.config(key, value)

        # Opci√≥n para habilitar Spark NLP si est√° en la configuraci√≥n
        use_spark_nlp = self.spark_config["spark"].get("use_spark_nlp", False)
        socketio = current_app.extensions["socketio"]

        if use_spark_nlp:
            socketio.emit("pipeline_output", {"message": f"Usando Spark NLP por defecto..."})
            self.spark = sparknlp.start()
        else:
            print("Iniciando Spark sin Spark NLP...")
            self.spark = spark_builder.getOrCreate()

        if self.debug:
            logs = []
            print("‚≠ê Sesi√≥n de Spark inicializada\n ")
            print(f"‚öôÔ∏è Configuraciones finales de Spark\n ")
            for key, value in self.spark.sparkContext.getConf().getAll():
                print(f"üîπ {key} = {value}")
                logs.append(f"üîπ {key} = {value}")
            return "\n".join(logs)
        else:
            return None
        
    def stop_pipeline(self):
        """Detiene la sesi√≥n de Spark."""
        if self.spark:
            self.spark.stop()
            print("Sesi√≥n de Spark detenida.")

    def apply_nlp_pipeline(self, df):
        """Aplica el modelo NLP en Spark"""
        if self.debug:
            print("‚öôÔ∏è Aplicando modelo de NLP...")

        pipeline_model_path = self.pipeline_config["models"].get("pipeline_path")
        if not pipeline_model_path:
            raise ValueError("No se especific√≥ un modelo en la configuraci√≥n.")

        pipeline_model = PipelineModel.load(pipeline_model_path)
        transformed_df = pipeline_model.transform(df)

        return transformed_df
    

    def get_spark_session(self):
        """Devuelve la sesi√≥n de Spark actual."""
        return self.spark

    def load_from_sql(self):
        """Carga los datos desde una base de datos SQL utilizando la consulta proporcionada."""
        if self.debug:
            print("üì° Cargando datos desde SQL...")

        # Extraer configuraci√≥n de la BD y consulta SQL
        db_config = self.input_data["database"]
        sql_query = self.input_data["query"]["sql"]
        
        db_type = db_config["type"]
        host = db_config["host"]
        port = db_config["port"]
        dbname = db_config["dbname"]
        user = db_config["user"]
        password = db_config["password"]

        if db_type == "postgresql":
            url = f"jdbc:postgresql://{host}:{port}/{dbname}"
            driver = "org.postgresql.Driver"
        elif db_type == "mysql":
            url = f"jdbc:mysql://{host}:{port}/{dbname}"
            driver = "com.mysql.cj.jdbc.Driver"
        elif db_type == "sqlserver":
            url = f"jdbc:sqlserver://{host}:{port};databaseName={dbname}"
            driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
        else:
            raise ValueError("Base de datos no soportada")

        if self.debug:
            print(f"Consulta SQL: {sql_query}")

        # Conexi√≥n JDBC a la base de datos
        df = self.spark.read.format("jdbc").options(
            url=url,
            query=sql_query,
            user=user,
            password=password,
            driver=driver
        ).load()

        if self.debug:
            print("‚úÖ Datos cargados correctamente desde SQL")
            df.show(5)

        return df


    def load_from_local(self, file_path: str, format: str = None, delimiter: str = ",") -> DataFrame:
        """
        üì• Carga un archivo desde el sistema de archivos local en un DataFrame de Spark.

        Par√°metros:
            - file_path (str): Ruta del archivo de entrada.
            - format (str, opcional): Formato del archivo. Si es None, se infiere por la extensi√≥n.
            - delimiter (str, opcional): Separador utilizado en archivos CSV y TXT. Predeterminado: ",".

        Retorna:
            - DataFrame: DataFrame de Spark con los datos cargados.
        """

        try:
            socketio = current_app.extensions["socketio"]
            socketio.emit("pipeline_output", {"message": f"üì• Cargando archivo desde {file_path}..."})

            if format is None:
                format = os.path.splitext(file_path)[-1].lower().replace(".", "")

            if format in ["csv", "txt"]:
                df = self.spark.read.option("header", "true").option("inferSchema", "true").option("delimiter", delimiter).csv(file_path)
            elif format == "json":
                df = self.spark.read.option("multiline", "true").json(file_path)
            elif format == "parquet":
                df = self.spark.read.parquet(file_path)
            elif format == "avro":
                df = self.spark.read.format("avro").load(file_path)
            elif format == "orc":
                df = self.spark.read.orc(file_path)
            elif format in ["xls", "xlsx"]:
                df = self.spark.read.format("com.crealytics.spark.excel") \
                    .option("header", "true") \
                    .option("inferSchema", "true") \
                    .load(file_path)
            else:
                raise ValueError(f"‚ùå Formato '{format}' no soportado. Usa 'csv', 'txt', 'json', 'parquet', 'avro', 'orc', 'xls', 'xlsx'.")

            return df

        except Exception as e:
            socketio.emit("pipeline_output", {"message": f"‚ùå Error al cargar el archivo: {str(e)}"})
            raise e

    def save_to_local(self, df: DataFrame, output_path: str, format: str = None, mode: str = "overwrite", delimiter: str = ","):
        """
        Guarda un DataFrame de Spark en un archivo local en distintos formatos.

        Par√°metros:
            - df (DataFrame): DataFrame de Spark a guardar.
            - output_path (str): Ruta donde se guardar√° el archivo.
            - format (str, opcional): Formato del archivo de salida. Si es None, se infiere por la extensi√≥n.
            - mode (str, opcional): Modo de guardado ('overwrite', 'append', 'error', 'ignore'). Predeterminado: 'overwrite'.
            - delimiter (str, opcional): Separador utilizado en archivos CSV y TXT. Predeterminado: ",".

        Retorna:
            - None
        """
        try:

            socketio = current_app.extensions["socketio"]

            if format is None:
                format = os.path.splitext(output_path)[-1].lower().replace(".", "")

            if format in ["csv", "txt"]:
                df.coalesce(1).write.option("header", "true").option("delimiter", delimiter).mode(mode).csv(output_path)
            elif format == "json":
                df.write.mode(mode).json(output_path)
            elif format == "parquet":
                df.write.mode(mode).parquet(output_path)
            elif format == "avro":
                df.write.format("avro").mode(mode).save(output_path)
            elif format == "orc":
                df.write.mode(mode).orc(output_path)
            elif format in ["xls", "xlsx"]:
                df.write.format("com.crealytics.spark.excel") \
                    .option("header", "true") \
                    .mode(mode) \
                    .save(output_path)
            else:
                raise ValueError(f"‚ùå Formato '{format}' no soportado. Usa 'csv', 'txt', 'json', 'parquet', 'avro', 'orc', 'xls', 'xlsx'.")

        except Exception as e:
            socketio.emit("pipeline_output", {"message": f"‚ùå Error al guardar el archivo: {str(e)}"})
            raise e


    def run(self, df: DataFrame) -> DataFrame:
        """Ejecuta el pipeline NLP y emite logs en tiempo real al cliente mediante WebSocket.""" 
        try:
            # Acceder a la instancia de Flask-SocketIO para emitir logs en tiempo real
            socketio = current_app.extensions["socketio"]

            # Inicio del procesamiento NLP
            socketio.emit("pipeline_output", {"message": "‚öôÔ∏è Iniciando procesamiento de NLP..."})

            # Mostrar la configuraci√≥n recibida si est√° en modo debug
            if self.debug:
                log_message = f"üì¢ Configuraci√≥n del pipeline:\n{json.dumps(self.pipeline_config, indent=2)}"
                socketio.emit("pipeline_output", {"message": log_message})

            # Verificar si la configuraci√≥n tiene la clave 'stages'
            if "stages" not in self.pipeline_config:
                socketio.emit("pipeline_output", {"message": "‚ùå Error: No se encontr√≥ 'stages' en la configuraci√≥n."})
                raise ValueError("‚ùå No se encontr√≥ 'stages' en la configuraci√≥n.")


            stages = []
            cached_models = {}  # Modelos cargados en memoria
            document_columns = []  # Columnas temporales generadas por el pipeline

            # üîÑ Construcci√≥n din√°mica del pipeline con cada etapa
            for stage in self.pipeline_config["stages"]:
                name = stage.get("name")
                params = stage.get("params", {})

                if not name:
                    socketio.emit("pipeline_output", {"message": "‚ùå Error: Falta 'name' en una etapa."})
                    raise ValueError("Falta 'name' en una etapa.")

                socketio.emit("pipeline_output", {"message": f"üì¢ Agregando etapa: {name} con par√°metros: {params}"})

                # üìÑ Document Assembler (Preprocesador de texto)
                if name == "document_assembler":
                    assembler = DocumentAssembler() \
                        .setInputCol(params["inputCol"]) \
                        .setOutputCol(params["outputCol"])
                    stages.append(assembler)
                    document_columns.append(params["outputCol"])

                # üåç Traducci√≥n con Marian Transformer
                elif name == "marian_transformer":
                    model_name = params["model_name"]
                    input_col = params["inputCol"]

                    # Si el modelo ya est√° en memoria, reutilizarlo
                    if model_name in cached_models:
                        socketio.emit("pipeline_output", {"message": f"üåç Modelo {model_name} ya cargado en memoria. Usando cach√©..."})
                        transformer = cached_models[model_name]
                    else:
                        # üì• Descargar y cargar el modelo si no est√° en cach√©
                        socketio.emit("pipeline_output", {"message": f"üåç Descargando modelo de traducci√≥n: {model_name}..."})
                        transformer = MarianTransformer.pretrained(model_name)  # Descargar y carga
                        cached_models[model_name] = transformer
                        socketio.emit("pipeline_output", {"message": f"üåç Modelo {model_name} descargado correctamente."})

                    transformer.setInputCols([input_col]).setOutputCol(params["outputCol"])
                    stages.append(transformer)

                # üîé Named Entity Recognition (NER)
                elif name == "ner_dl":
                    model_name = params["model_name"]
                    input_col = params["inputCol"]
                    output_col = params["outputCol"]

                    # i el modelo ya est√° en memoria, reutilizarlo
                    if model_name in cached_models:
                        socketio.emit("pipeline_output", {"message": f"üîç Modelo NER {model_name} ya cargado en memoria. Usando cach√©..."})
                        ner_model = cached_models[model_name]
                    else:
                        # üì• Descargar y cargar el modelo si no est√° en cach√©
                        socketio.emit("pipeline_output", {"message": f"üîç Descargando modelo NER: {model_name}..."})
                        ner_model = NerDLModel.pretrained(model_name, "en")
                        cached_models[model_name] = ner_model
                        socketio.emit("pipeline_output", {"message": f"üîç Modelo NER {model_name} descargado correctamente."})

                    ner_model.setInputCols([input_col]).setOutputCol(output_col)
                    stages.append(ner_model)

                # üèÅ Finisher para convertir estructuras de Spark NLP en texto plano
                elif name == "finisher":
                    input_cols = params.get("inputCols", [])
                    output_cols = params.get("outputCols", input_cols)
                    include_metadata = params.get("includeMetadata", False)
                    output_as_array = params.get("outputAsArray", False)

                    finisher = Finisher() \
                        .setInputCols(input_cols) \
                        .setOutputCols(output_cols) \
                        .setIncludeMetadata(include_metadata) \
                        .setOutputAsArray(output_as_array)

                    stages.append(finisher)

                # Fin de bucle. ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Si se recibe una etapa desconocida, lanzar un error ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
                else:
                    socketio.emit("pipeline_output", {"message": f"‚ùå Error: Tipo de etapa '{name}' no soportado."})
                    raise ValueError(f"Tipo de etapa '{name}' no soportado.")

            # ‚ùå Si no hay etapas v√°lidas, lanzar error
            if not stages:
                socketio.emit("pipeline_output", {"message": "‚ùå Error: No hay etapas v√°lidas en el pipeline."})
                raise ValueError("No hay etapas v√°lidas en el pipeline.")

            # üöÄ Creaci√≥n y ejecuci√≥n del pipeline en Spark
            socketio.emit("pipeline_output", {"message": "üöÄ Ejecutando pipeline en Spark..."})
            nlp_pipeline = Pipeline(stages=stages)

            socketio.emit("pipeline_output", {"message": "üöÄ Ajustando (fit)..."} )
            model = nlp_pipeline.fit(df)

            socketio.emit("pipeline_output", {"message": "üöÄ Transformando..."} )
            transformed_df = model.transform(df)

            # Eliminar columnas temporales generadas
            socketio.emit("pipeline_output", {"message": "üßπLimpiando columnas temporales..."})
            for col in document_columns:
                if col in transformed_df.columns:
                    transformed_df = transformed_df.drop(col)

            # Finalizaci√≥n del pipeline
            socketio.emit("pipeline_output", {"message": "‚úÖ Transformaci√≥n NLP completada üëç "})

        except Exception as e:
            # ‚ùå En caso de error, emitir mensaje al cliente y detener la ejecuci√≥n del pipeline
            socketio.emit("pipeline_output", {"message": f"üò± ‚ùå Error durante la ejecuci√≥n del pipeline: {str(e)} ‚ùåüòû"})
            raise e

        return transformed_df
